# Wittgenbot üóø

<span style="font-size: 18px;">
    <img src="/public/readme_image.png" alt="Sample Image" style="float: right; margin-right: 10px; width: 50%">
    Welcome to <b>Wittgenbot</b>, your chatbot companion for exploring Ludwig Wittgenstein's philosophy.
</span>
<br>

## Instructions üìã

<span style="font-size: 18px;">

Simply select a model to chat with, ask away, and let your philosophical confusions dissolve. üí≠

Below are some sample questions related to Wittgenstein's work that you could ask <strong>Wittgenbot</strong>:
<ul style="list-style-type: square;">
  <li><em>What determines the meaning of a word?</em></li>
  <li><em>When can something be classified as a game?</em></li>
  <li><em>Is the existence of a private language possible?</em></li>
  <li><em>How do philosophical problems arise from misunderstandings of language?</em></li>
</ul>
</span>

## Models üß†

<span style="font-size: 18px;">

We offer the following models:

- **WITT-FT**: A fine-tuned model with
  - A [dataset](https://huggingface.co/datasets/descartesevildemon/Ludwig-Wittgenstein-QA-Pairs) of over *3000* Q-A pairs generated by [Mixtral&#8209;8x7B&#8209;v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1).
  - Base model [Mistral&#8209;7B&#8209;Instruct&#8209;v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF) fine-tuned on the Q-A dataset.
  - Longest inference time but most accurate and well-structured answers.<br><br>

- **WITT-0.5K**: Utilizes a RAG chain with
  - Context length (<span style="font-size: 16px;">`chunk_size`</span>) of *512* tokens and overlap (<span style="font-size: 16px;">`chunk_overlap`</span>) of *50* tokens.
  - Model [Mistral&#8209;7B&#8209;Instruct&#8209;v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF).
  - Short inference time but higher risk of inaccurate answers.<br><br>

- **WITT-1.5K**: Utilizes a RAG chain with
  - Context length (<span style="font-size: 16px;">`chunk_size`</span>) of *1500* tokens and overlap (<span style="font-size: 16px;">`chunk_overlap`</span>) of *300* tokens.
  - Model [Mistral&#8209;7B&#8209;Instruct&#8209;v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF).
  - Long inference time but lower risk of inaccurate answers.<br><br>

---

## Fine-Tuning üîß

<span style="font-size: 18px;">

The **WITT-FT** model was fine-tuned on over 3000 question-answer (Q-A) pairs that were generated by [Mixtral&#8209;8x7B&#8209;v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1). With the help of a philosophy expert, [Mistral&#8209;7B&#8209;Instruct&#8209;v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF) was chosen as the base model due to its coherent and conversational tone. The fine-tuned model was primarily evaluated manually by the expert to accomodate for the project's niche application.

</span>

### Dataset üìö

<span style="font-size: 18px;">

The data generation model was chosen as [Mixtral&#8209;8x7B&#8209;v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) due to the structure of its answers and its coherent tone. The questions and answers were generated separately in order to provide more detailed and customized instructions for each process.

The [dataset](https://huggingface.co/datasets/descartesevildemon/Ludwig-Wittgenstein-QA-Pairs) creation process began with creating an exhaustive list of *all* topics, from major ideas to demonstrative examples, in Ludwig Wittgenstein's two main works: **_Tractatus Logico-Philosophicus_ (1921)** and **_Philosophical Investigations_ (1953)**, in order to guide the Q-A generation. This list, developed through manual effort and **GPT&#8209;4**, was verified by a philosophy expert to ensure the incorporation of human verification in the dataset creation process.

Then, for each topic, a list of comprehensive questions was generated by [Mixtral&#8209;8x7B&#8209;v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1). These questions were then fed into [Mixtral&#8209;8x7B&#8209;v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) to generate the answers in a manner that a philosophy professor would for a confused student. This amounted to a [dataset](https://huggingface.co/datasets/descartesevildemon/Ludwig-Wittgenstein-QA-Pairs) with a total of 3144 Q-A pairs.

</span>
&nbsp;

---

## Retrieval Augmented Generation (RAG) üîÑ

<span style="font-size: 18px;">

The **WITT-0.5K** and **WITT-1.5K** models are based on **Retrieval Augmented Generation (RAG)** with a collection of academic secondary texts as their knowledge base. The secondary texts were partitioned into chunks of size <span style="font-size: 16px;">`chunk_size`</span> tokens with consecutive chunks overlapping by <span style="font-size: 16px;">`chunk_overlap`</span> tokens. The chunks were then embedded into vectors of dimensionality 384 using the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) embedding model and stored in a vector database. The RAG models use this database as a context reference before answering the question through an LLM, [Mistral&#8209;7B&#8209;Instruct&#8209;v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF).<br>
The question-answer procedure is outlined below:
1. The user asks a question.
2. The question is embedded into a vector and *mathematically* compared to other vectors in the database, where the 25 most similar vectors (chunks) are collected. Equivalently, 25 excerpts relevant to the question are collected.
3. The 25 relevant excerpts, along with the question, are fed into a reranking model by [Cohere](https://cohere.com/), which *semantically* reranks them in terms of relevance to the question, and the top 3 are selected.
4. The question and 3 most relevant excerpts, each of size <span style="font-size: 16px;">`chunk_size`</span> tokens, are fed into an LLM, **Mistral-7B-Instruct-v0.2**, which generates an answer using the excerpts as context.

</span>

### Dataset üìö

<span style="font-size: 18px;">

Academic secondary sources about Ludwig Wittgenstein's **_Tractatus Logico-Philosophicus_ (1921)** and **_Philosophical Investigations_ (1953)** were collected in bulk via [CORE API](https://core.ac.uk/services/api). Unlike fine-tuning, RAG allows for a cheap expansion of the models' knowledge-bases by simply collecting more relevant secondary sources through CORE and storing them in the vector database.

</span>
&nbsp;

---

## Final Year Project üóÇÔ∏è

<span style="font-size: 18px;">

This project was done by **Ali Jaafar**, **Nadim Akkaoui** and **Shoushy Kojayan** as their Final Year Project.

</span>

## GitHub Repository üîÆ

<span style="font-size: 18px;">

The code of the entire project is available on [this GitHub organization](https://github.com/Wittgenbot).

</span>